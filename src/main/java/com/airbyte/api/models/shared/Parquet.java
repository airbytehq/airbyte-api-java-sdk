/* 
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

package com.airbyte.api.models.shared;

import com.airbyte.api.utils.LazySingletonValue;
import com.airbyte.api.utils.Utils;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonFormat;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.core.type.TypeReference;
import java.io.InputStream;
import java.lang.Deprecated;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.Optional;

/**
 * Parquet - This connector utilises &lt;a href="https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetFile.html" target="_blank"&gt;PyArrow (Apache Arrow)&lt;/a&gt; for Parquet parsing.
 */

public class Parquet {

    /**
     * Maximum number of records per batch read from the input files. Batches may be smaller if there aren’t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("batch_size")
    private Optional<? extends Long> batchSize;

    /**
     * Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("buffer_size")
    private Optional<? extends Long> bufferSize;

    /**
     * If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("columns")
    private Optional<? extends java.util.List<String>> columns;

    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("filetype")
    private Optional<? extends SourceS3Filetype> filetype;

    @JsonCreator
    public Parquet(
            @JsonProperty("batch_size") Optional<? extends Long> batchSize,
            @JsonProperty("buffer_size") Optional<? extends Long> bufferSize,
            @JsonProperty("columns") Optional<? extends java.util.List<String>> columns) {
        Utils.checkNotNull(batchSize, "batchSize");
        Utils.checkNotNull(bufferSize, "bufferSize");
        Utils.checkNotNull(columns, "columns");
        this.batchSize = batchSize;
        this.bufferSize = bufferSize;
        this.columns = columns;
        this.filetype = Builder._SINGLETON_VALUE_Filetype.value();
    }
    
    public Parquet() {
        this(Optional.empty(), Optional.empty(), Optional.empty());
    }

    /**
     * Maximum number of records per batch read from the input files. Batches may be smaller if there aren’t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<Long> batchSize() {
        return (Optional<Long>) batchSize;
    }

    /**
     * Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<Long> bufferSize() {
        return (Optional<Long>) bufferSize;
    }

    /**
     * If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<java.util.List<String>> columns() {
        return (Optional<java.util.List<String>>) columns;
    }

    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<SourceS3Filetype> filetype() {
        return (Optional<SourceS3Filetype>) filetype;
    }

    public final static Builder builder() {
        return new Builder();
    }

    /**
     * Maximum number of records per batch read from the input files. Batches may be smaller if there aren’t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    public Parquet withBatchSize(long batchSize) {
        Utils.checkNotNull(batchSize, "batchSize");
        this.batchSize = Optional.ofNullable(batchSize);
        return this;
    }

    /**
     * Maximum number of records per batch read from the input files. Batches may be smaller if there aren’t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    public Parquet withBatchSize(Optional<? extends Long> batchSize) {
        Utils.checkNotNull(batchSize, "batchSize");
        this.batchSize = batchSize;
        return this;
    }

    /**
     * Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    public Parquet withBufferSize(long bufferSize) {
        Utils.checkNotNull(bufferSize, "bufferSize");
        this.bufferSize = Optional.ofNullable(bufferSize);
        return this;
    }

    /**
     * Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
     */
    public Parquet withBufferSize(Optional<? extends Long> bufferSize) {
        Utils.checkNotNull(bufferSize, "bufferSize");
        this.bufferSize = bufferSize;
        return this;
    }

    /**
     * If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
     */
    public Parquet withColumns(java.util.List<String> columns) {
        Utils.checkNotNull(columns, "columns");
        this.columns = Optional.ofNullable(columns);
        return this;
    }

    /**
     * If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
     */
    public Parquet withColumns(Optional<? extends java.util.List<String>> columns) {
        Utils.checkNotNull(columns, "columns");
        this.columns = columns;
        return this;
    }
    
    @Override
    public boolean equals(java.lang.Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        Parquet other = (Parquet) o;
        return 
            java.util.Objects.deepEquals(this.batchSize, other.batchSize) &&
            java.util.Objects.deepEquals(this.bufferSize, other.bufferSize) &&
            java.util.Objects.deepEquals(this.columns, other.columns) &&
            java.util.Objects.deepEquals(this.filetype, other.filetype);
    }
    
    @Override
    public int hashCode() {
        return java.util.Objects.hash(
            batchSize,
            bufferSize,
            columns,
            filetype);
    }
    
    @Override
    public String toString() {
        return Utils.toString(Parquet.class,
                "batchSize", batchSize,
                "bufferSize", bufferSize,
                "columns", columns,
                "filetype", filetype);
    }
    
    public final static class Builder {
 
        private Optional<? extends Long> batchSize;
 
        private Optional<? extends Long> bufferSize;
 
        private Optional<? extends java.util.List<String>> columns = Optional.empty();  
        
        private Builder() {
          // force use of static builder() method
        }

        /**
         * Maximum number of records per batch read from the input files. Batches may be smaller if there aren’t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
         */
        public Builder batchSize(long batchSize) {
            Utils.checkNotNull(batchSize, "batchSize");
            this.batchSize = Optional.ofNullable(batchSize);
            return this;
        }

        /**
         * Maximum number of records per batch read from the input files. Batches may be smaller if there aren’t enough rows in the file. This option can help avoid out-of-memory errors if your data is particularly wide.
         */
        public Builder batchSize(Optional<? extends Long> batchSize) {
            Utils.checkNotNull(batchSize, "batchSize");
            this.batchSize = batchSize;
            return this;
        }

        /**
         * Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
         */
        public Builder bufferSize(long bufferSize) {
            Utils.checkNotNull(bufferSize, "bufferSize");
            this.bufferSize = Optional.ofNullable(bufferSize);
            return this;
        }

        /**
         * Perform read buffering when deserializing individual column chunks. By default every group column will be loaded fully to memory. This option can help avoid out-of-memory errors if your data is particularly wide.
         */
        public Builder bufferSize(Optional<? extends Long> bufferSize) {
            Utils.checkNotNull(bufferSize, "bufferSize");
            this.bufferSize = bufferSize;
            return this;
        }

        /**
         * If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
         */
        public Builder columns(java.util.List<String> columns) {
            Utils.checkNotNull(columns, "columns");
            this.columns = Optional.ofNullable(columns);
            return this;
        }

        /**
         * If you only want to sync a subset of the columns from the file(s), add the columns you want here as a comma-delimited list. Leave it empty to sync all columns.
         */
        public Builder columns(Optional<? extends java.util.List<String>> columns) {
            Utils.checkNotNull(columns, "columns");
            this.columns = columns;
            return this;
        }
        
        public Parquet build() {
            if (batchSize == null) {
                batchSize = _SINGLETON_VALUE_BatchSize.value();
            }
            if (bufferSize == null) {
                bufferSize = _SINGLETON_VALUE_BufferSize.value();
            }
            return new Parquet(
                batchSize,
                bufferSize,
                columns);
        }

        private static final LazySingletonValue<Optional<? extends Long>> _SINGLETON_VALUE_BatchSize =
                new LazySingletonValue<>(
                        "batch_size",
                        "65536",
                        new TypeReference<Optional<? extends Long>>() {});

        private static final LazySingletonValue<Optional<? extends Long>> _SINGLETON_VALUE_BufferSize =
                new LazySingletonValue<>(
                        "buffer_size",
                        "2",
                        new TypeReference<Optional<? extends Long>>() {});

        private static final LazySingletonValue<Optional<? extends SourceS3Filetype>> _SINGLETON_VALUE_Filetype =
                new LazySingletonValue<>(
                        "filetype",
                        "\"parquet\"",
                        new TypeReference<Optional<? extends SourceS3Filetype>>() {});
    }
}

