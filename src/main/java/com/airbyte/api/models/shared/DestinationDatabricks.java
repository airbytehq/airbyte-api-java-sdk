/* 
 * Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT.
 */

package com.airbyte.api.models.shared;


import com.airbyte.api.utils.LazySingletonValue;
import com.airbyte.api.utils.Utils;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.core.type.TypeReference;
import java.lang.Boolean;
import java.lang.Override;
import java.lang.String;
import java.util.Objects;
import java.util.Optional;


public class DestinationDatabricks {

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("accept_terms")
    private Optional<Boolean> acceptTerms;

    /**
     * Authentication mechanism for Staging files and running queries
     */
    @JsonProperty("authentication")
    private Authentication authentication;

    /**
     * The name of the unity catalog for the database
     */
    @JsonProperty("database")
    private String database;

    @JsonProperty("destinationType")
    private Databricks destinationType;

    /**
     * Databricks Cluster Server Hostname.
     */
    @JsonProperty("hostname")
    private String hostname;

    /**
     * Databricks Cluster HTTP Path.
     */
    @JsonProperty("http_path")
    private String httpPath;

    /**
     * Databricks Cluster Port.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("port")
    private Optional<String> port;

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("purge_staging_data")
    private Optional<Boolean> purgeStagingData;

    /**
     * The schema to write raw tables into (default: airbyte_internal)
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("raw_schema_override")
    private Optional<String> rawSchemaOverride;

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("schema")
    private Optional<String> schema;

    @JsonCreator
    public DestinationDatabricks(
            @JsonProperty("accept_terms") Optional<Boolean> acceptTerms,
            @JsonProperty("authentication") Authentication authentication,
            @JsonProperty("database") String database,
            @JsonProperty("hostname") String hostname,
            @JsonProperty("http_path") String httpPath,
            @JsonProperty("port") Optional<String> port,
            @JsonProperty("purge_staging_data") Optional<Boolean> purgeStagingData,
            @JsonProperty("raw_schema_override") Optional<String> rawSchemaOverride,
            @JsonProperty("schema") Optional<String> schema) {
        Utils.checkNotNull(acceptTerms, "acceptTerms");
        Utils.checkNotNull(authentication, "authentication");
        Utils.checkNotNull(database, "database");
        Utils.checkNotNull(hostname, "hostname");
        Utils.checkNotNull(httpPath, "httpPath");
        Utils.checkNotNull(port, "port");
        Utils.checkNotNull(purgeStagingData, "purgeStagingData");
        Utils.checkNotNull(rawSchemaOverride, "rawSchemaOverride");
        Utils.checkNotNull(schema, "schema");
        this.acceptTerms = acceptTerms;
        this.authentication = authentication;
        this.database = database;
        this.destinationType = Builder._SINGLETON_VALUE_DestinationType.value();
        this.hostname = hostname;
        this.httpPath = httpPath;
        this.port = port;
        this.purgeStagingData = purgeStagingData;
        this.rawSchemaOverride = rawSchemaOverride;
        this.schema = schema;
    }
    
    public DestinationDatabricks(
            Authentication authentication,
            String database,
            String hostname,
            String httpPath) {
        this(Optional.empty(), authentication, database, hostname, httpPath, Optional.empty(), Optional.empty(), Optional.empty(), Optional.empty());
    }

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    @JsonIgnore
    public Optional<Boolean> acceptTerms() {
        return acceptTerms;
    }

    /**
     * Authentication mechanism for Staging files and running queries
     */
    @JsonIgnore
    public Authentication authentication() {
        return authentication;
    }

    /**
     * The name of the unity catalog for the database
     */
    @JsonIgnore
    public String database() {
        return database;
    }

    @JsonIgnore
    public Databricks destinationType() {
        return destinationType;
    }

    /**
     * Databricks Cluster Server Hostname.
     */
    @JsonIgnore
    public String hostname() {
        return hostname;
    }

    /**
     * Databricks Cluster HTTP Path.
     */
    @JsonIgnore
    public String httpPath() {
        return httpPath;
    }

    /**
     * Databricks Cluster Port.
     */
    @JsonIgnore
    public Optional<String> port() {
        return port;
    }

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    @JsonIgnore
    public Optional<Boolean> purgeStagingData() {
        return purgeStagingData;
    }

    /**
     * The schema to write raw tables into (default: airbyte_internal)
     */
    @JsonIgnore
    public Optional<String> rawSchemaOverride() {
        return rawSchemaOverride;
    }

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    @JsonIgnore
    public Optional<String> schema() {
        return schema;
    }

    public final static Builder builder() {
        return new Builder();
    }

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    public DestinationDatabricks withAcceptTerms(boolean acceptTerms) {
        Utils.checkNotNull(acceptTerms, "acceptTerms");
        this.acceptTerms = Optional.ofNullable(acceptTerms);
        return this;
    }

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    public DestinationDatabricks withAcceptTerms(Optional<Boolean> acceptTerms) {
        Utils.checkNotNull(acceptTerms, "acceptTerms");
        this.acceptTerms = acceptTerms;
        return this;
    }

    /**
     * Authentication mechanism for Staging files and running queries
     */
    public DestinationDatabricks withAuthentication(Authentication authentication) {
        Utils.checkNotNull(authentication, "authentication");
        this.authentication = authentication;
        return this;
    }

    /**
     * The name of the unity catalog for the database
     */
    public DestinationDatabricks withDatabase(String database) {
        Utils.checkNotNull(database, "database");
        this.database = database;
        return this;
    }

    /**
     * Databricks Cluster Server Hostname.
     */
    public DestinationDatabricks withHostname(String hostname) {
        Utils.checkNotNull(hostname, "hostname");
        this.hostname = hostname;
        return this;
    }

    /**
     * Databricks Cluster HTTP Path.
     */
    public DestinationDatabricks withHttpPath(String httpPath) {
        Utils.checkNotNull(httpPath, "httpPath");
        this.httpPath = httpPath;
        return this;
    }

    /**
     * Databricks Cluster Port.
     */
    public DestinationDatabricks withPort(String port) {
        Utils.checkNotNull(port, "port");
        this.port = Optional.ofNullable(port);
        return this;
    }

    /**
     * Databricks Cluster Port.
     */
    public DestinationDatabricks withPort(Optional<String> port) {
        Utils.checkNotNull(port, "port");
        this.port = port;
        return this;
    }

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    public DestinationDatabricks withPurgeStagingData(boolean purgeStagingData) {
        Utils.checkNotNull(purgeStagingData, "purgeStagingData");
        this.purgeStagingData = Optional.ofNullable(purgeStagingData);
        return this;
    }

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    public DestinationDatabricks withPurgeStagingData(Optional<Boolean> purgeStagingData) {
        Utils.checkNotNull(purgeStagingData, "purgeStagingData");
        this.purgeStagingData = purgeStagingData;
        return this;
    }

    /**
     * The schema to write raw tables into (default: airbyte_internal)
     */
    public DestinationDatabricks withRawSchemaOverride(String rawSchemaOverride) {
        Utils.checkNotNull(rawSchemaOverride, "rawSchemaOverride");
        this.rawSchemaOverride = Optional.ofNullable(rawSchemaOverride);
        return this;
    }

    /**
     * The schema to write raw tables into (default: airbyte_internal)
     */
    public DestinationDatabricks withRawSchemaOverride(Optional<String> rawSchemaOverride) {
        Utils.checkNotNull(rawSchemaOverride, "rawSchemaOverride");
        this.rawSchemaOverride = rawSchemaOverride;
        return this;
    }

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    public DestinationDatabricks withSchema(String schema) {
        Utils.checkNotNull(schema, "schema");
        this.schema = Optional.ofNullable(schema);
        return this;
    }

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    public DestinationDatabricks withSchema(Optional<String> schema) {
        Utils.checkNotNull(schema, "schema");
        this.schema = schema;
        return this;
    }
    
    @Override
    public boolean equals(java.lang.Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        DestinationDatabricks other = (DestinationDatabricks) o;
        return 
            Objects.deepEquals(this.acceptTerms, other.acceptTerms) &&
            Objects.deepEquals(this.authentication, other.authentication) &&
            Objects.deepEquals(this.database, other.database) &&
            Objects.deepEquals(this.destinationType, other.destinationType) &&
            Objects.deepEquals(this.hostname, other.hostname) &&
            Objects.deepEquals(this.httpPath, other.httpPath) &&
            Objects.deepEquals(this.port, other.port) &&
            Objects.deepEquals(this.purgeStagingData, other.purgeStagingData) &&
            Objects.deepEquals(this.rawSchemaOverride, other.rawSchemaOverride) &&
            Objects.deepEquals(this.schema, other.schema);
    }
    
    @Override
    public int hashCode() {
        return Objects.hash(
            acceptTerms,
            authentication,
            database,
            destinationType,
            hostname,
            httpPath,
            port,
            purgeStagingData,
            rawSchemaOverride,
            schema);
    }
    
    @Override
    public String toString() {
        return Utils.toString(DestinationDatabricks.class,
                "acceptTerms", acceptTerms,
                "authentication", authentication,
                "database", database,
                "destinationType", destinationType,
                "hostname", hostname,
                "httpPath", httpPath,
                "port", port,
                "purgeStagingData", purgeStagingData,
                "rawSchemaOverride", rawSchemaOverride,
                "schema", schema);
    }
    
    public final static class Builder {
 
        private Optional<Boolean> acceptTerms;
 
        private Authentication authentication;
 
        private String database;
 
        private String hostname;
 
        private String httpPath;
 
        private Optional<String> port;
 
        private Optional<Boolean> purgeStagingData;
 
        private Optional<String> rawSchemaOverride;
 
        private Optional<String> schema;  
        
        private Builder() {
          // force use of static builder() method
        }

        /**
         * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
         */
        public Builder acceptTerms(boolean acceptTerms) {
            Utils.checkNotNull(acceptTerms, "acceptTerms");
            this.acceptTerms = Optional.ofNullable(acceptTerms);
            return this;
        }

        /**
         * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
         */
        public Builder acceptTerms(Optional<Boolean> acceptTerms) {
            Utils.checkNotNull(acceptTerms, "acceptTerms");
            this.acceptTerms = acceptTerms;
            return this;
        }

        /**
         * Authentication mechanism for Staging files and running queries
         */
        public Builder authentication(Authentication authentication) {
            Utils.checkNotNull(authentication, "authentication");
            this.authentication = authentication;
            return this;
        }

        /**
         * The name of the unity catalog for the database
         */
        public Builder database(String database) {
            Utils.checkNotNull(database, "database");
            this.database = database;
            return this;
        }

        /**
         * Databricks Cluster Server Hostname.
         */
        public Builder hostname(String hostname) {
            Utils.checkNotNull(hostname, "hostname");
            this.hostname = hostname;
            return this;
        }

        /**
         * Databricks Cluster HTTP Path.
         */
        public Builder httpPath(String httpPath) {
            Utils.checkNotNull(httpPath, "httpPath");
            this.httpPath = httpPath;
            return this;
        }

        /**
         * Databricks Cluster Port.
         */
        public Builder port(String port) {
            Utils.checkNotNull(port, "port");
            this.port = Optional.ofNullable(port);
            return this;
        }

        /**
         * Databricks Cluster Port.
         */
        public Builder port(Optional<String> port) {
            Utils.checkNotNull(port, "port");
            this.port = port;
            return this;
        }

        /**
         * Default to 'true'. Switch it to 'false' for debugging purpose.
         */
        public Builder purgeStagingData(boolean purgeStagingData) {
            Utils.checkNotNull(purgeStagingData, "purgeStagingData");
            this.purgeStagingData = Optional.ofNullable(purgeStagingData);
            return this;
        }

        /**
         * Default to 'true'. Switch it to 'false' for debugging purpose.
         */
        public Builder purgeStagingData(Optional<Boolean> purgeStagingData) {
            Utils.checkNotNull(purgeStagingData, "purgeStagingData");
            this.purgeStagingData = purgeStagingData;
            return this;
        }

        /**
         * The schema to write raw tables into (default: airbyte_internal)
         */
        public Builder rawSchemaOverride(String rawSchemaOverride) {
            Utils.checkNotNull(rawSchemaOverride, "rawSchemaOverride");
            this.rawSchemaOverride = Optional.ofNullable(rawSchemaOverride);
            return this;
        }

        /**
         * The schema to write raw tables into (default: airbyte_internal)
         */
        public Builder rawSchemaOverride(Optional<String> rawSchemaOverride) {
            Utils.checkNotNull(rawSchemaOverride, "rawSchemaOverride");
            this.rawSchemaOverride = rawSchemaOverride;
            return this;
        }

        /**
         * The default schema tables are written. If not specified otherwise, the "default" will be used.
         */
        public Builder schema(String schema) {
            Utils.checkNotNull(schema, "schema");
            this.schema = Optional.ofNullable(schema);
            return this;
        }

        /**
         * The default schema tables are written. If not specified otherwise, the "default" will be used.
         */
        public Builder schema(Optional<String> schema) {
            Utils.checkNotNull(schema, "schema");
            this.schema = schema;
            return this;
        }
        
        public DestinationDatabricks build() {
            if (acceptTerms == null) {
                acceptTerms = _SINGLETON_VALUE_AcceptTerms.value();
            }
            if (port == null) {
                port = _SINGLETON_VALUE_Port.value();
            }
            if (purgeStagingData == null) {
                purgeStagingData = _SINGLETON_VALUE_PurgeStagingData.value();
            }
            if (rawSchemaOverride == null) {
                rawSchemaOverride = _SINGLETON_VALUE_RawSchemaOverride.value();
            }
            if (schema == null) {
                schema = _SINGLETON_VALUE_Schema.value();
            }            return new DestinationDatabricks(
                acceptTerms,
                authentication,
                database,
                hostname,
                httpPath,
                port,
                purgeStagingData,
                rawSchemaOverride,
                schema);
        }

        private static final LazySingletonValue<Optional<Boolean>> _SINGLETON_VALUE_AcceptTerms =
                new LazySingletonValue<>(
                        "accept_terms",
                        "false",
                        new TypeReference<Optional<Boolean>>() {});

        private static final LazySingletonValue<Databricks> _SINGLETON_VALUE_DestinationType =
                new LazySingletonValue<>(
                        "destinationType",
                        "\"databricks\"",
                        new TypeReference<Databricks>() {});

        private static final LazySingletonValue<Optional<String>> _SINGLETON_VALUE_Port =
                new LazySingletonValue<>(
                        "port",
                        "\"443\"",
                        new TypeReference<Optional<String>>() {});

        private static final LazySingletonValue<Optional<Boolean>> _SINGLETON_VALUE_PurgeStagingData =
                new LazySingletonValue<>(
                        "purge_staging_data",
                        "true",
                        new TypeReference<Optional<Boolean>>() {});

        private static final LazySingletonValue<Optional<String>> _SINGLETON_VALUE_RawSchemaOverride =
                new LazySingletonValue<>(
                        "raw_schema_override",
                        "\"airbyte_internal\"",
                        new TypeReference<Optional<String>>() {});

        private static final LazySingletonValue<Optional<String>> _SINGLETON_VALUE_Schema =
                new LazySingletonValue<>(
                        "schema",
                        "\"default\"",
                        new TypeReference<Optional<String>>() {});
    }
}

