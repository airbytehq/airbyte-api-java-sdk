/* 
 * Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.
 */

package com.airbyte.api.models.shared;

import com.airbyte.api.utils.LazySingletonValue;
import com.airbyte.api.utils.Utils;
import com.fasterxml.jackson.annotation.JsonCreator;
import com.fasterxml.jackson.annotation.JsonFormat;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonInclude.Include;
import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;
import com.fasterxml.jackson.core.type.TypeReference;
import java.io.InputStream;
import java.lang.Deprecated;
import java.math.BigDecimal;
import java.math.BigInteger;
import java.util.Optional;


public class DestinationDatabricks {

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("accept_terms")
    private Optional<? extends Boolean> acceptTerms;

    /**
     * Storage on which the delta lake is built.
     */
    @JsonProperty("data_source")
    private DataSource dataSource;

    /**
     * The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("database")
    private Optional<? extends String> database;

    /**
     * Databricks Cluster HTTP Path.
     */
    @JsonProperty("databricks_http_path")
    private String databricksHttpPath;

    /**
     * Databricks Personal Access Token for making authenticated requests.
     */
    @JsonProperty("databricks_personal_access_token")
    private String databricksPersonalAccessToken;

    /**
     * Databricks Cluster Port.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("databricks_port")
    private Optional<? extends String> databricksPort;

    /**
     * Databricks Cluster Server Hostname.
     */
    @JsonProperty("databricks_server_hostname")
    private String databricksServerHostname;

    @JsonProperty("destinationType")
    private Databricks destinationType;

    /**
     * Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("enable_schema_evolution")
    private Optional<? extends Boolean> enableSchemaEvolution;

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("purge_staging_data")
    private Optional<? extends Boolean> purgeStagingData;

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    @JsonInclude(Include.NON_ABSENT)
    @JsonProperty("schema")
    private Optional<? extends String> schema;

    @JsonCreator
    public DestinationDatabricks(
            @JsonProperty("accept_terms") Optional<? extends Boolean> acceptTerms,
            @JsonProperty("data_source") DataSource dataSource,
            @JsonProperty("database") Optional<? extends String> database,
            @JsonProperty("databricks_http_path") String databricksHttpPath,
            @JsonProperty("databricks_personal_access_token") String databricksPersonalAccessToken,
            @JsonProperty("databricks_port") Optional<? extends String> databricksPort,
            @JsonProperty("databricks_server_hostname") String databricksServerHostname,
            @JsonProperty("enable_schema_evolution") Optional<? extends Boolean> enableSchemaEvolution,
            @JsonProperty("purge_staging_data") Optional<? extends Boolean> purgeStagingData,
            @JsonProperty("schema") Optional<? extends String> schema) {
        Utils.checkNotNull(acceptTerms, "acceptTerms");
        Utils.checkNotNull(dataSource, "dataSource");
        Utils.checkNotNull(database, "database");
        Utils.checkNotNull(databricksHttpPath, "databricksHttpPath");
        Utils.checkNotNull(databricksPersonalAccessToken, "databricksPersonalAccessToken");
        Utils.checkNotNull(databricksPort, "databricksPort");
        Utils.checkNotNull(databricksServerHostname, "databricksServerHostname");
        Utils.checkNotNull(enableSchemaEvolution, "enableSchemaEvolution");
        Utils.checkNotNull(purgeStagingData, "purgeStagingData");
        Utils.checkNotNull(schema, "schema");
        this.acceptTerms = acceptTerms;
        this.dataSource = dataSource;
        this.database = database;
        this.databricksHttpPath = databricksHttpPath;
        this.databricksPersonalAccessToken = databricksPersonalAccessToken;
        this.databricksPort = databricksPort;
        this.databricksServerHostname = databricksServerHostname;
        this.destinationType = Builder._SINGLETON_VALUE_DestinationType.value();
        this.enableSchemaEvolution = enableSchemaEvolution;
        this.purgeStagingData = purgeStagingData;
        this.schema = schema;
    }
    
    public DestinationDatabricks(
            DataSource dataSource,
            String databricksHttpPath,
            String databricksPersonalAccessToken,
            String databricksServerHostname) {
        this(Optional.empty(), dataSource, Optional.empty(), databricksHttpPath, databricksPersonalAccessToken, Optional.empty(), databricksServerHostname, Optional.empty(), Optional.empty(), Optional.empty());
    }

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<Boolean> acceptTerms() {
        return (Optional<Boolean>) acceptTerms;
    }

    /**
     * Storage on which the delta lake is built.
     */
    @JsonIgnore
    public DataSource dataSource() {
        return dataSource;
    }

    /**
     * The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<String> database() {
        return (Optional<String>) database;
    }

    /**
     * Databricks Cluster HTTP Path.
     */
    @JsonIgnore
    public String databricksHttpPath() {
        return databricksHttpPath;
    }

    /**
     * Databricks Personal Access Token for making authenticated requests.
     */
    @JsonIgnore
    public String databricksPersonalAccessToken() {
        return databricksPersonalAccessToken;
    }

    /**
     * Databricks Cluster Port.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<String> databricksPort() {
        return (Optional<String>) databricksPort;
    }

    /**
     * Databricks Cluster Server Hostname.
     */
    @JsonIgnore
    public String databricksServerHostname() {
        return databricksServerHostname;
    }

    @JsonIgnore
    public Databricks destinationType() {
        return destinationType;
    }

    /**
     * Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<Boolean> enableSchemaEvolution() {
        return (Optional<Boolean>) enableSchemaEvolution;
    }

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<Boolean> purgeStagingData() {
        return (Optional<Boolean>) purgeStagingData;
    }

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    @SuppressWarnings("unchecked")
    @JsonIgnore
    public Optional<String> schema() {
        return (Optional<String>) schema;
    }

    public final static Builder builder() {
        return new Builder();
    }

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    public DestinationDatabricks withAcceptTerms(boolean acceptTerms) {
        Utils.checkNotNull(acceptTerms, "acceptTerms");
        this.acceptTerms = Optional.ofNullable(acceptTerms);
        return this;
    }

    /**
     * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
     */
    public DestinationDatabricks withAcceptTerms(Optional<? extends Boolean> acceptTerms) {
        Utils.checkNotNull(acceptTerms, "acceptTerms");
        this.acceptTerms = acceptTerms;
        return this;
    }

    /**
     * Storage on which the delta lake is built.
     */
    public DestinationDatabricks withDataSource(DataSource dataSource) {
        Utils.checkNotNull(dataSource, "dataSource");
        this.dataSource = dataSource;
        return this;
    }

    /**
     * The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
     */
    public DestinationDatabricks withDatabase(String database) {
        Utils.checkNotNull(database, "database");
        this.database = Optional.ofNullable(database);
        return this;
    }

    /**
     * The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
     */
    public DestinationDatabricks withDatabase(Optional<? extends String> database) {
        Utils.checkNotNull(database, "database");
        this.database = database;
        return this;
    }

    /**
     * Databricks Cluster HTTP Path.
     */
    public DestinationDatabricks withDatabricksHttpPath(String databricksHttpPath) {
        Utils.checkNotNull(databricksHttpPath, "databricksHttpPath");
        this.databricksHttpPath = databricksHttpPath;
        return this;
    }

    /**
     * Databricks Personal Access Token for making authenticated requests.
     */
    public DestinationDatabricks withDatabricksPersonalAccessToken(String databricksPersonalAccessToken) {
        Utils.checkNotNull(databricksPersonalAccessToken, "databricksPersonalAccessToken");
        this.databricksPersonalAccessToken = databricksPersonalAccessToken;
        return this;
    }

    /**
     * Databricks Cluster Port.
     */
    public DestinationDatabricks withDatabricksPort(String databricksPort) {
        Utils.checkNotNull(databricksPort, "databricksPort");
        this.databricksPort = Optional.ofNullable(databricksPort);
        return this;
    }

    /**
     * Databricks Cluster Port.
     */
    public DestinationDatabricks withDatabricksPort(Optional<? extends String> databricksPort) {
        Utils.checkNotNull(databricksPort, "databricksPort");
        this.databricksPort = databricksPort;
        return this;
    }

    /**
     * Databricks Cluster Server Hostname.
     */
    public DestinationDatabricks withDatabricksServerHostname(String databricksServerHostname) {
        Utils.checkNotNull(databricksServerHostname, "databricksServerHostname");
        this.databricksServerHostname = databricksServerHostname;
        return this;
    }

    /**
     * Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
     */
    public DestinationDatabricks withEnableSchemaEvolution(boolean enableSchemaEvolution) {
        Utils.checkNotNull(enableSchemaEvolution, "enableSchemaEvolution");
        this.enableSchemaEvolution = Optional.ofNullable(enableSchemaEvolution);
        return this;
    }

    /**
     * Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
     */
    public DestinationDatabricks withEnableSchemaEvolution(Optional<? extends Boolean> enableSchemaEvolution) {
        Utils.checkNotNull(enableSchemaEvolution, "enableSchemaEvolution");
        this.enableSchemaEvolution = enableSchemaEvolution;
        return this;
    }

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    public DestinationDatabricks withPurgeStagingData(boolean purgeStagingData) {
        Utils.checkNotNull(purgeStagingData, "purgeStagingData");
        this.purgeStagingData = Optional.ofNullable(purgeStagingData);
        return this;
    }

    /**
     * Default to 'true'. Switch it to 'false' for debugging purpose.
     */
    public DestinationDatabricks withPurgeStagingData(Optional<? extends Boolean> purgeStagingData) {
        Utils.checkNotNull(purgeStagingData, "purgeStagingData");
        this.purgeStagingData = purgeStagingData;
        return this;
    }

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    public DestinationDatabricks withSchema(String schema) {
        Utils.checkNotNull(schema, "schema");
        this.schema = Optional.ofNullable(schema);
        return this;
    }

    /**
     * The default schema tables are written. If not specified otherwise, the "default" will be used.
     */
    public DestinationDatabricks withSchema(Optional<? extends String> schema) {
        Utils.checkNotNull(schema, "schema");
        this.schema = schema;
        return this;
    }
    
    @Override
    public boolean equals(java.lang.Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        DestinationDatabricks other = (DestinationDatabricks) o;
        return 
            java.util.Objects.deepEquals(this.acceptTerms, other.acceptTerms) &&
            java.util.Objects.deepEquals(this.dataSource, other.dataSource) &&
            java.util.Objects.deepEquals(this.database, other.database) &&
            java.util.Objects.deepEquals(this.databricksHttpPath, other.databricksHttpPath) &&
            java.util.Objects.deepEquals(this.databricksPersonalAccessToken, other.databricksPersonalAccessToken) &&
            java.util.Objects.deepEquals(this.databricksPort, other.databricksPort) &&
            java.util.Objects.deepEquals(this.databricksServerHostname, other.databricksServerHostname) &&
            java.util.Objects.deepEquals(this.destinationType, other.destinationType) &&
            java.util.Objects.deepEquals(this.enableSchemaEvolution, other.enableSchemaEvolution) &&
            java.util.Objects.deepEquals(this.purgeStagingData, other.purgeStagingData) &&
            java.util.Objects.deepEquals(this.schema, other.schema);
    }
    
    @Override
    public int hashCode() {
        return java.util.Objects.hash(
            acceptTerms,
            dataSource,
            database,
            databricksHttpPath,
            databricksPersonalAccessToken,
            databricksPort,
            databricksServerHostname,
            destinationType,
            enableSchemaEvolution,
            purgeStagingData,
            schema);
    }
    
    @Override
    public String toString() {
        return Utils.toString(DestinationDatabricks.class,
                "acceptTerms", acceptTerms,
                "dataSource", dataSource,
                "database", database,
                "databricksHttpPath", databricksHttpPath,
                "databricksPersonalAccessToken", databricksPersonalAccessToken,
                "databricksPort", databricksPort,
                "databricksServerHostname", databricksServerHostname,
                "destinationType", destinationType,
                "enableSchemaEvolution", enableSchemaEvolution,
                "purgeStagingData", purgeStagingData,
                "schema", schema);
    }
    
    public final static class Builder {
 
        private Optional<? extends Boolean> acceptTerms;
 
        private DataSource dataSource;
 
        private Optional<? extends String> database = Optional.empty();
 
        private String databricksHttpPath;
 
        private String databricksPersonalAccessToken;
 
        private Optional<? extends String> databricksPort;
 
        private String databricksServerHostname;
 
        private Optional<? extends Boolean> enableSchemaEvolution;
 
        private Optional<? extends Boolean> purgeStagingData;
 
        private Optional<? extends String> schema;  
        
        private Builder() {
          // force use of static builder() method
        }

        /**
         * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
         */
        public Builder acceptTerms(boolean acceptTerms) {
            Utils.checkNotNull(acceptTerms, "acceptTerms");
            this.acceptTerms = Optional.ofNullable(acceptTerms);
            return this;
        }

        /**
         * You must agree to the Databricks JDBC Driver &lt;a href="https://databricks.com/jdbc-odbc-driver-license"&gt;Terms &amp; Conditions&lt;/a&gt; to use this connector.
         */
        public Builder acceptTerms(Optional<? extends Boolean> acceptTerms) {
            Utils.checkNotNull(acceptTerms, "acceptTerms");
            this.acceptTerms = acceptTerms;
            return this;
        }

        /**
         * Storage on which the delta lake is built.
         */
        public Builder dataSource(DataSource dataSource) {
            Utils.checkNotNull(dataSource, "dataSource");
            this.dataSource = dataSource;
            return this;
        }

        /**
         * The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
         */
        public Builder database(String database) {
            Utils.checkNotNull(database, "database");
            this.database = Optional.ofNullable(database);
            return this;
        }

        /**
         * The name of the catalog. If not specified otherwise, the "hive_metastore" will be used.
         */
        public Builder database(Optional<? extends String> database) {
            Utils.checkNotNull(database, "database");
            this.database = database;
            return this;
        }

        /**
         * Databricks Cluster HTTP Path.
         */
        public Builder databricksHttpPath(String databricksHttpPath) {
            Utils.checkNotNull(databricksHttpPath, "databricksHttpPath");
            this.databricksHttpPath = databricksHttpPath;
            return this;
        }

        /**
         * Databricks Personal Access Token for making authenticated requests.
         */
        public Builder databricksPersonalAccessToken(String databricksPersonalAccessToken) {
            Utils.checkNotNull(databricksPersonalAccessToken, "databricksPersonalAccessToken");
            this.databricksPersonalAccessToken = databricksPersonalAccessToken;
            return this;
        }

        /**
         * Databricks Cluster Port.
         */
        public Builder databricksPort(String databricksPort) {
            Utils.checkNotNull(databricksPort, "databricksPort");
            this.databricksPort = Optional.ofNullable(databricksPort);
            return this;
        }

        /**
         * Databricks Cluster Port.
         */
        public Builder databricksPort(Optional<? extends String> databricksPort) {
            Utils.checkNotNull(databricksPort, "databricksPort");
            this.databricksPort = databricksPort;
            return this;
        }

        /**
         * Databricks Cluster Server Hostname.
         */
        public Builder databricksServerHostname(String databricksServerHostname) {
            Utils.checkNotNull(databricksServerHostname, "databricksServerHostname");
            this.databricksServerHostname = databricksServerHostname;
            return this;
        }

        /**
         * Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
         */
        public Builder enableSchemaEvolution(boolean enableSchemaEvolution) {
            Utils.checkNotNull(enableSchemaEvolution, "enableSchemaEvolution");
            this.enableSchemaEvolution = Optional.ofNullable(enableSchemaEvolution);
            return this;
        }

        /**
         * Support schema evolution for all streams. If "false", the connector might fail when a stream's schema changes.
         */
        public Builder enableSchemaEvolution(Optional<? extends Boolean> enableSchemaEvolution) {
            Utils.checkNotNull(enableSchemaEvolution, "enableSchemaEvolution");
            this.enableSchemaEvolution = enableSchemaEvolution;
            return this;
        }

        /**
         * Default to 'true'. Switch it to 'false' for debugging purpose.
         */
        public Builder purgeStagingData(boolean purgeStagingData) {
            Utils.checkNotNull(purgeStagingData, "purgeStagingData");
            this.purgeStagingData = Optional.ofNullable(purgeStagingData);
            return this;
        }

        /**
         * Default to 'true'. Switch it to 'false' for debugging purpose.
         */
        public Builder purgeStagingData(Optional<? extends Boolean> purgeStagingData) {
            Utils.checkNotNull(purgeStagingData, "purgeStagingData");
            this.purgeStagingData = purgeStagingData;
            return this;
        }

        /**
         * The default schema tables are written. If not specified otherwise, the "default" will be used.
         */
        public Builder schema(String schema) {
            Utils.checkNotNull(schema, "schema");
            this.schema = Optional.ofNullable(schema);
            return this;
        }

        /**
         * The default schema tables are written. If not specified otherwise, the "default" will be used.
         */
        public Builder schema(Optional<? extends String> schema) {
            Utils.checkNotNull(schema, "schema");
            this.schema = schema;
            return this;
        }
        
        public DestinationDatabricks build() {
            if (acceptTerms == null) {
                acceptTerms = _SINGLETON_VALUE_AcceptTerms.value();
            }
            if (databricksPort == null) {
                databricksPort = _SINGLETON_VALUE_DatabricksPort.value();
            }
            if (enableSchemaEvolution == null) {
                enableSchemaEvolution = _SINGLETON_VALUE_EnableSchemaEvolution.value();
            }
            if (purgeStagingData == null) {
                purgeStagingData = _SINGLETON_VALUE_PurgeStagingData.value();
            }
            if (schema == null) {
                schema = _SINGLETON_VALUE_Schema.value();
            }
            return new DestinationDatabricks(
                acceptTerms,
                dataSource,
                database,
                databricksHttpPath,
                databricksPersonalAccessToken,
                databricksPort,
                databricksServerHostname,
                enableSchemaEvolution,
                purgeStagingData,
                schema);
        }

        private static final LazySingletonValue<Optional<? extends Boolean>> _SINGLETON_VALUE_AcceptTerms =
                new LazySingletonValue<>(
                        "accept_terms",
                        "false",
                        new TypeReference<Optional<? extends Boolean>>() {});

        private static final LazySingletonValue<Optional<? extends String>> _SINGLETON_VALUE_DatabricksPort =
                new LazySingletonValue<>(
                        "databricks_port",
                        "\"443\"",
                        new TypeReference<Optional<? extends String>>() {});

        private static final LazySingletonValue<Databricks> _SINGLETON_VALUE_DestinationType =
                new LazySingletonValue<>(
                        "destinationType",
                        "\"databricks\"",
                        new TypeReference<Databricks>() {});

        private static final LazySingletonValue<Optional<? extends Boolean>> _SINGLETON_VALUE_EnableSchemaEvolution =
                new LazySingletonValue<>(
                        "enable_schema_evolution",
                        "false",
                        new TypeReference<Optional<? extends Boolean>>() {});

        private static final LazySingletonValue<Optional<? extends Boolean>> _SINGLETON_VALUE_PurgeStagingData =
                new LazySingletonValue<>(
                        "purge_staging_data",
                        "true",
                        new TypeReference<Optional<? extends Boolean>>() {});

        private static final LazySingletonValue<Optional<? extends String>> _SINGLETON_VALUE_Schema =
                new LazySingletonValue<>(
                        "schema",
                        "\"default\"",
                        new TypeReference<Optional<? extends String>>() {});
    }
}

